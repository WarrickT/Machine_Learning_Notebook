{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5176c2",
   "metadata": {},
   "source": [
    "# Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98fcfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063ec42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.linear_q = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_k = nn.Linear(input_size, hidden_size)\n",
    "        self.linear_v = nn.Linear(input_size, hidden_size) \n",
    "        self.linear_x = nn.Linear(input_size, hidden_size)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q, k, v = self.linear_q(x), self.linear_k(x), self.linear_v(x)\n",
    "        x = self.norm(self.linear_x(x) + self.attention(q, k, v))\n",
    "        x = self.norm(x + self.fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af699b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TweetTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_class):\n",
    "        super(TweetTransformer, self).__init__() \n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.encoder = TransformerEncoder(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "    \n",
    "    def forward(self, x, pos):\n",
    "        x = self.emb(x) + pos \n",
    "        x = self.encoder(x)\n",
    "\n",
    "        x = torch.sum(x, -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e3ae2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
