{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc31292d",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478336a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c47daf61",
   "metadata": {},
   "source": [
    "Unlike SARSA and Q_Learning, Policy Gradients directly optimize the agent's policy by estimating the gradient of the expected reward with respect to policy parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039bb10",
   "metadata": {},
   "source": [
    "This is a policy-based method. It does NOT rely on a value function. The policy is expressed in parametrized form. \n",
    "\n",
    "When the policy is implemented using a neural network, the policy parameters refer to the network weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa371e7",
   "metadata": {},
   "source": [
    "## The Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77682c12",
   "metadata": {},
   "source": [
    "By definition this is the \n",
    "\n",
    "**Expection (over the trajectory $\\tau$ generated by following the policy ${\\pi}_{\\theta}$ in each step ) of the returns $R(\\tau)$ generated over the trajectory $\\tau$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194e78f",
   "metadata": {},
   "source": [
    "## The Gradient of the Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413cff7",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\nabla}_{\\theta}J({\\pi}_{\\theta}) = {\\nabla}_{\\theta}\\mathbb{E}_{{\\tau} ~ {\\pi}_{\\theta}}[R(\\tau)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122ab626",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
